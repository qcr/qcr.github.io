{"pageProps":{"datasetData":{"content":"<p>The RT-GENE dataset is a novel dataset of varied gaze and head pose images in a natural environment, addressing the issue of ground truth annotation by measuring head pose using a motion capture system and eye gaze using mobile eyetracking glasses. We apply semantic image inpainting to the area covered by the glasses to bridge the gap between training and testing images by removing the obtrusiveness of the glasses. The proposed RT-GENE dataset contains recordings of 15 participants (9 male, 6 female, 2 participants recorded twice), with a total of 122,531 labeled training images and 154,755 unlabeled images of the same subjects where the eyetracking glasses are not worn.</p>\n<p><picture><img alt=\"RT-GENE dataset\" src=\"https://github.com/Tobias-Fischer/rt_gene/raw/master/assets/dataset_collection_setup.jpg\"></picture></p>\n<p>The work done in this project was done within the <a href=\"https://www.imperial.ac.uk/personal-robotics\">Personal Robotics Lab at Imperial College London</a>.</p>\n","name":"RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments Dataset","type":"dataset","url":"https://zenodo.org/record/2529036","url_type":"external","size":"45GB","id":"rt_gene_dataset","image":"repo:Tobias-Fischer/rt_gene/assets/dataset_figure.jpg","image_fit":"contain","_images":["/_next/static/images/dataset_figure-5572954dcf83fca94ae80fa38a0f36ab.jpg.webp","/_next/static/images/dataset_figure-024bff6ee75c09b3b9afd020a4e1467b.jpg"],"src":"/content/rt-gene/rt-gene-dataset.md","image_position":"center"}},"__N_SSG":true}