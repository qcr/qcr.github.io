{"pageProps":{"codeData":{"content":"<h1>VPR-Bench</h1>\n<h2>What is VPR-Bench</h2>\n<p>VPR-Bench is an open-source Visual Place Recognition evaluation framework with quantifiable viewpoint and illumination invariance. This repository represents the open-source release relating to our VPR-Bench paper published in the International Journal of Computer Vision, which you can access <a href=\"https://doi.org/10.1007/s11263-021-01469-5\">here</a>.\n<picture><img alt=\"VPR-Bench Block Diagram\" src=\"https:/github.com/MubarizZaffar/VPR-Bench/raw/HEAD/VPRBench.jpg\"></picture></p>\n<p>This repository allows you to do the following two things:</p>\n<ol>\n<li>\n<p>Compute the performance of 8 VPR techniques on 12 VPR datasets using multiple evaluation metrics, such as PR curves, ROC curves, RecallRate@N, True-Positive Distribution over a Trajectory etc.</p>\n</li>\n<li>\n<p>Compute the quantified limits of viewpoint and illumination invariance of VPR techniques on Point Features dataset, QUT Multi-lane dataset and MIT Multi-illumination dataset.</p>\n</li>\n</ol>\n<h3>List of Techniques</h3>\n<ol>\n<li>NetVLAD [R. Arandjelović et al; https://arxiv.org/abs/1511.07247]</li>\n<li>RegionVLAD [Khaliq et al; https://ieeexplore.ieee.org/document/8944012]</li>\n<li>CoHOG [Zaffar et al; https://ieeexplore.ieee.org/document/8972582]</li>\n<li>HOG [Dalal at al; OpenCV Implementation]</li>\n<li>AlexNet [Krizhevsky et al; https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf]</li>\n<li>AMOSNet [Chen et al; https://ieeexplore.ieee.org/document/7989366]</li>\n<li>HybridNet [Chen et al; https://ieeexplore.ieee.org/document/7989366]</li>\n<li>CALC [Merrill et al; http://www.roboticsproceedings.org/rss14/p32.pdf]</li>\n<li>DenseVLAD (Results-only) [Torii et al; https://ieeexplore.ieee.org/document/7298790]</li>\n<li>AP-GeM (Results-only) [Revaud et al; https://ieeexplore.ieee.org/document/9010047]</li>\n</ol>\n<h3>List of Datasets</h3>\n<ol>\n<li>ESSEX3IN1 [Zaffar et al; https://ieeexplore.ieee.org/document/9126220]</li>\n<li>Tokyo24/7 [R. Arandjelović et al; https://arxiv.org/abs/1511.07247]</li>\n<li>SPEDTest [Chen et al; https://ieeexplore.ieee.org/document/8421024]</li>\n<li>Synthia [Ros et al; https://ieeexplore.ieee.org/document/7780721]</li>\n<li>Nordland [Skrede et al; https://bit.ly/2QVBOym]</li>\n<li>Gardens Point [Glover et al; https://doi.org/10.5281/zenodo.4590133]</li>\n<li>INRIA Holidays [Jegou et al; https://lear.inrialpes.fr/pubs/2008/JDS08/jegou_hewgc08.pdf]</li>\n<li>Pittsburgh Query [R. Arandjelović et al; https://arxiv.org/abs/1511.07247]</li>\n<li>Cross-Seasons [Larsson et al; https://ieeexplore.ieee.org/document/8953253]</li>\n<li>Corridor [Milford et al; https://journals.sagepub.com/doi/abs/10.1177/0278364913490323]</li>\n<li>Living Room [Milford et al; https://ieeexplore.ieee.org/document/7487686]</li>\n<li>17 Places [Sahdev et al; https://ieeexplore.ieee.org/document/7801503]</li>\n</ol>\n<p>Side Note: You can extend our codebase to include more datasets (or use full versions of some datasets) and techniques by following the templates described in the appendix of our paper. For further understanding these templates, dig into the 'VPR_techniques' and 'helper_functions' folders of this repository.</p>\n<h2>Dependencies</h2>\n<p>Our code was written in Python 2, tested in Ubuntu 18.04 LTS and Ubuntu 20.04 LTS both using Anaconda Python. Please follow the below steps for installing dependencies:</p>\n<ol>\n<li>\n<p>Install Anaconda Python on your system (https://docs.anaconda.com/anaconda/install/). We are running conda 4.9.2 but other versions should also work.</p>\n</li>\n<li>\n<p>Clone this VPR-Bench Github repository (using git clone).</p>\n</li>\n</ol>\n<pre><code>git clone https://github.com/MubarizZaffar/VPR-Bench\n\n</code></pre>\n<ol start=\"3\">\n<li>Using 'cd' change your working directory to the downloaded VPR-Bench repository and execute the shell script 'must_downloads.sh'. This will download, extract and copy all the required model files and variation quantified datasets into their respective folders.</li>\n</ol>\n<pre><code>cd YOURDIR/VPR-Bench/\nsh must_downloads.sh\n</code></pre>\n<ol start=\"4\">\n<li>This VPR-Bench repository also contains a YAML file named 'environment.yml'. Using this file, you can create a new conda environment (named 'myvprbenchenv') containing all the dependencies by running the following in your terminal.</li>\n</ol>\n<pre><code>conda env create -f environment.yml\n</code></pre>\n<ol start=\"5\">\n<li>\n<p>There is a known Caffe bug regarding 'mean shape incompatible with input shape' , so follow the solution in https://stackoverflow.com/questions/30808735/error-when-using-classify-in-caffe. That is, modify the lines 253-254 in {USER}/anaconda3/envs/myvprbenchenv/lib/python2.7/site-packages/caffe.</p>\n</li>\n<li>\n<p>Finally activate your environment using the following and you should be good to go.</p>\n</li>\n</ol>\n<pre><code>\nconda activate myvprbenchenv\n\n</code></pre>\n<ol start=\"7\">\n<li>(Backup) If for some reason you are unable to create a conda environment from environment.yml, please look into the 'VPR_Bench_dependencies_installationcommands.txt' file in this repo, which specifies the individual commands needed to install the dependencies for VPR-Bench in a fresh Python 2 conda environment. A similar backup file namely 'must_downloads.txt' has also been provided for the 'must_downloads.sh' shell script.</li>\n</ol>\n<h2>Using VPR-Bench</h2>\n<ul>\n<li>After activating the 'myvprbenchenv' environment, execute the following in your terminal. This will compute the VPR performance of CoHOG and CALC on Corridor dataset while storing PR curves, matching information, RecallRate curves and others in respective sub-folders within the VPR-Bench folder (i.e. the downloaded Github repo).</li>\n</ul>\n<pre><code>python main.py -em 0 -sm 1 -dn Corridor -ddir datasets/corridor/ -mdir precomputed_matches/corridor/ -techs CoHOG CALC\n</code></pre>\n<ul>\n<li>If you want to use any of the other 8 VPR techniques, modify the arguments in terminal accordingly e.g. to use all 8 VPR techniques, see below.</li>\n</ul>\n<pre><code>python main.py -em 0 -sm 1 -dn Corridor -ddir datasets/corridor/ -mdir precomputed_matches/corridor/ -techs CoHOG CALC NetVLAD RegionVLAD AMOSNet HybridNet HOG AlexNet_VPR\n</code></pre>\n<ul>\n<li>\n<p>If you want to use any of the other 12 datasets in our work, download them from here (https://surfdrive.surf.nl/files/index.php/s/sbZRXzYe3l0v67W), and set the dataset fields (-dn and -ddir) accordingly.</p>\n</li>\n<li>\n<p>If you just want to use the matching data we had already computed for the 10 techniques on 12 datasets in our work, append '_Precomputed' to the name of a technique(s). This matching info is already available for corridor and SPEDTEST datasets in this repo, but for other datasets you would need to have downloaded this matching data (https://surfdrive.surf.nl/files/index.php/s/ThIgFycwwhRCVZv). Also set the dataset path via -ddir for access to ground-truth data. This ground-truth data is present for all datasets by default in the 'VPR-Bench/datasets/' folder. An example usage is given below.</p>\n</li>\n</ul>\n<pre><code>python main.py -em 0 -sm 0 -dn SPEDTEST -ddir datasets/SPEDTEST/ -mdir precomputed_matches/SPEDTEST/ -techs CoHOG_Precomputed CALC_Precomputed NetVLAD_Precomputed RegionVLAD_Precomputed\n</code></pre>\n<ul>\n<li>If you want to run the viewpoint and illumination invariance analysis of our work, change the 'VPR_evaluation_mode' (-em) in main.py to 1/2/3 (by default it is 0), to get this analysis on Point Features dataset, QUT Multi-lane dataset and MIT Multi-illumination dataset, respectively. Example execution is following.</li>\n</ul>\n<pre><code>python main.py -em 2 -techs NetVLAD RegionVLAD AMOSNet HybridNet CALC HOG CoHOG AlexNet_VPR\n\n</code></pre>\n<h1>Related External Resources</h1>\n<h2>Datasets</h2>\n<ul>\n<li>Tokyo 24/7:https://github.com/Relja/netvlad</li>\n<li>17 Places: https://www.raghavendersahdev.com/place-recognition.html</li>\n</ul>\n<h2>Techniques</h2>\n<ul>\n<li>AP-GeM: https://github.com/naver/deep-image-retrieval</li>\n<li>DenseVLAD: http://www.ok.ctrl.titech.ac.jp/~torii/project/247/</li>\n</ul>\n<h1>Contacts</h1>\n<p>You can send an email at mubarizzaffar at gmail dot com, m dot zaffar at tudelft dot nl or s dot garg at qut dot edu dot au for further guidance and/or questions.</p>\n<p>Important Note: For all the datasets and techniques, we have made our maximum effort to provide original citations and/or licenses within the respective folders, where possible and applicable. We request all users of VPR-Bench to be aware of (and use) the original citations and licenses in any of their works. If you have any concerns about this, please do send us an email.</p>\n<h1>Cite as</h1>\n<p>If you find this work useful, please cite as:</p>\n<pre><code>@article{zaffar2021vpr,\n  title={Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change},\n  author={Zaffar, Mubariz and Garg, Sourav and Milford, Michael and Kooij, Julian and Flynn, David and McDonald-Maier, Klaus and Ehsan, Shoaib},\n  journal={International Journal of Computer Vision},\n  pages={1--39},\n  year={2021},\n  publisher={Springer}\n}\n</code></pre>\n","name":"VPR-Bench","type":"code","url":"https://github.com/MubarizZaffar/VPR-Bench","id":"vprbench","image":"VPRBench.jpg","_images":["/_next/static/images/VPRBench-a4fbe919a2ac5fc851261353f3fbdd9a.jpg.webp","/_next/static/images/VPRBench-5db45a25afa26692b0958cbf579b9a77.jpg"],"src":"/content/visual_place_recognition/vprbench_code.md","image_position":"center"}},"__N_SSG":true}