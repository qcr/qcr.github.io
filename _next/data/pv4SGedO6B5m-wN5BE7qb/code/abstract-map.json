{"pageProps":{"codeData":{"content":"<p align=\"center\"><strong>~ Please see the <a href=\"https://btalb.github.io/abstract_map/\">abstract map site</a> for further details about the research publication ~</strong></p>\n<h1>The Abstract Map - using symbols to navigate</h1>\n<p><picture><img alt=\"The abstract map in action\" src=\"https:/github.com/btalb/abstract_map/raw/HEAD/docs/assets/images/abstract_map_in_action.png\"></picture></p>\n<p>This repository provides the implementation of the abstract map used in our <a href=\"https://doi.org/10.1109/TCDS.2020.2993855\">IEEE TCDS journal</a>. The implementation, done in Python, includes the following features:</p>\n<ul>\n<li>a novel dynamics-based malleable spatial model for imagining unseen spaces from symbols (which includes simulated springs, friction, repulsive forces, &amp; collision models)</li>\n<li>a visualiser &amp; text-based commentator for introspection of your navigation system (both shown in videos on the <a href=\"https://btalb.github.io/abstract_map/\">repository website</a>)</li>\n<li>easy ROS bindings for getting up &amp; running in simulation or on a real robot</li>\n<li>tag readers &amp; interpreters for extracting symbolic spatial information from <a href=\"http://wiki.ros.org/apriltag_ros\">AprilTags</a></li>\n<li>configuration files for the zoo experiments performed on GP-S11 of QUT's Gardens Point campus (see <a href=\"https://doi.org/10.1109/TCDS.2020.2993855\">the paper</a> for further details)</li>\n<li>serialisation methods for passing an entire abstract map state between machines, or saving to file</li>\n</ul>\n<p>Please see our other related repositories for further resources, and related parts of the abstract map studies:</p>\n<ul>\n<li><strong><a href=\"https://github.com/btalb/abstract_map_simulator\">abstract_map_simulator</a>:</strong> all of the resources needed to run a full 2D simulation of the zoo experiments performed on GP-S11 of our Gardens Point campus at QUT</li>\n<li><strong><a href=\"https://github.com/btalb/abstract_map_app\">abstract_map_app</a>:</strong> mobile Android application used by human participants to complete navigation tasks as part of the zoo experiments (the app used the on-board camera to scan tags &amp; present the mapped symbolic spatial information in real time)</li>\n</ul>\n<h2>Getting up &amp; running with the abstract map</h2>\n<p><em>Note: if you wish to run this in simulation (significantly easier than on a real robot platform), you will also need the <a href=\"https://github.com/btalb/abstract_map_simulator\">abstract_map_simulator</a> package</em></p>\n<h3>Setting up your environment</h3>\n<p>Clone the repo &amp; install all Python dependencies:</p>\n<pre><code>git clone https://github.com/btalb/abstract_map\npip install -r abstract_map/requirements.txt\n</code></pre>\n<p>Add the new package to your ROS workspace at <code>&lt;ROS_WS&gt;/</code> by linking in the cloned repository:</p>\n<pre><code>ln -s &lt;LOCATION_REPO_WAS_CLONED_ABOVE&gt; &lt;ROS_WS&gt;/src/\n</code></pre>\n<p>Install all of the listed ROS dependencies, and build the package:</p>\n<pre><code>cd &lt;ROS_WS&gt;/src/\nrosdep install abstract_map\ncd &lt;ROS_WS&gt;\ncatkin_make\n</code></pre>\n<h3>Running the Zoo experiments</h3>\n<p>Start the experiment (this will try &amp; launch the 2D simulation back-end by default, so make sure you have that installed if you are using it):</p>\n<pre><code>roslaunch abstract_map experiment.launch\n</code></pre>\n<p><em>(please see <a href=\"https://github.com/btalb/abstract_map_simulator/issues/1\">this issue</a> for details if you get the spam of TF based errors... which probably shouldn't even be errors... )</em></p>\n<p>In another terminal, start the hierarchy publisher to give the abstract map the contextual symbolic spatial information to begin with:</p>\n<pre><code>rosrun abstract_map hierarchy_publisher\n</code></pre>\n<p>This will use the hierarchy available in <code>./experiments/zoo_hierarchy.xml</code> by default. Feel free to make your own if you would like to do different experiments.</p>\n<p>Start the visualiser in preparation of beginning the experiment (pick either light or dark mode with one of the two commands):</p>\n<pre><code>rosrun abstract_map visualiser\n</code></pre>\n<pre><code>rosrun abstract_map visualiser --dark\n</code></pre>\n<p><picture><img alt=\"Visualise the abstract map with dark or light colours\" src=\"https:/github.com/btalb/abstract_map/raw/HEAD/docs/assets/images/abstract_map_light_vs_dark.png\"></picture></p>\n<p>Finally, start the abstract map with a goal, and watch it attempt to complete the navigation task:</p>\n<pre><code>roslaunch abstract_map abstract_map.launch goal:=Lion\n</code></pre>\n<p>If you want to manually drive the robot around and observe how the abstract map evolves over time, you can run the above command without a goal to start in \"observe mode\".</p>\n<h2>Acknowledgements &amp; Citing our work</h2>\n<p>This work was supported by the Australian Research Council's Discovery Projects Funding Scheme under Project DP140103216. The authors are with the <a href=\"https://research.qut.edu.au/qcr/\">QUT Centre for Robotics</a>.</p>\n<p>If you use this software in your research, or for comparisons, please kindly cite our work:</p>\n<pre><code>@ARTICLE{9091567,  \n    author={B. {Talbot} and F. {Dayoub} and P. {Corke} and G. {Wyeth}},  \n    journal={IEEE Transactions on Cognitive and Developmental Systems},   \n    title={Robot Navigation in Unseen Spaces using an Abstract Map},   \n    year={2020},  \n    volume={},  \n    number={},  \n    pages={1-1},\n    keywords={Navigation;Robot sensing systems;Measurement;Linguistics;Visualization;symbol grounding;symbolic spatial information;abstract map;navigation;cognitive robotics;intelligent robots.},\n    doi={10.1109/TCDS.2020.2993855},\n    ISSN={2379-8939},\n    month={},}\n}\n</code></pre>\n","name":"Abstract Map (Python)","type":"code","url":"https://github.com/btalb/abstract_map","image":"./docs/assets/images/abstract_map_in_action.png","_images":["/_next/static/images/abstract_map_in_action-51c5e1dcb68134fbb20baad53816b40f.png.webp","/_next/static/images/abstract_map_in_action-863c3403cb5be611fa8f5dcbdbb45c3f.png"],"src":"/content/human_cues/abstract-map.md","id":"abstract-map","image_position":"center"}},"__N_SSG":true}