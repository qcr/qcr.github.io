{"pageProps":{"codeData":{"content":"<h1>LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics</h1>\n<p>This is the source code for the paper titled - \"LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics\", [<a href=\"https://arxiv.org/abs/1804.05526\">arXiv</a>][<a href=\"http://www.roboticsproceedings.org/rss14/p22.pdf\">RSS 2018 Proceedings</a>]</p>\n<p>An example output image showing Keypoint Correspondences:</p>\n<p><picture><img alt=\"An example output image showing Keypoint Correspondences\" src=\"https:/github.com/oravus/lostX/raw/HEAD/lost_kc/bin/day-night-keypoint-correspondence-place-recognition.jpg\"></picture></p>\n<p>Flowchart of the proposed approach:</p>\n<p><picture><img alt=\"Flowchart of the proposed approach\" src=\"https:/github.com/oravus/lostX/raw/HEAD/lost_kc/bin/LoST-Flowchart-Visual_Place_Recognition.jpg\"></picture></p>\n<p>If you find this work useful, please cite it as:<br>\nSourav Garg, Niko Sunderhauf, and Michael Milford. LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics. Proceedings of Robotics: Science and Systems XIV, 2018.<br>\nbibtex:</p>\n<pre><code>@article{garg2018lost,\ntitle={LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints using Visual Semantics},\nauthor={Garg, Sourav and Suenderhauf, Niko and Milford, Michael},\njournal={Proceedings of Robotics: Science and Systems XIV},\nyear={2018}\n}\n</code></pre>\n<p>RefineNet's citation as mentioned on their <a href=\"https://github.com/guosheng/refinenet\">Github page</a>.</p>\n<h2>Setup and Run</h2>\n<h4>Dependencies</h4>\n<ul>\n<li>Ubuntu        (Tested on <em>14.04</em>)</li>\n<li><a href=\"https://arxiv.org/abs/1611.06612\">RefineNet</a>\n<ul>\n<li>Required primarily for visual semantic information. Convolutional feature maps based dense descriptors are also extracted from the same.</li>\n<li>A <a href=\"https://github.com/oravus/refinenet\">modified fork</a> of RefineNet's <a href=\"https://github.com/guosheng/refinenet\">code</a> is used in this work to simultaneously store convolutional dense descriptors.</li>\n<li>Requires Matlab      (Tested on <em>2017a</em>)</li>\n</ul>\n</li>\n<li>Python        (Tested on <em>2.7</em>)\n<ul>\n<li>numpy       (Tested on <em>1.11.1</em>, <em>1.14.2</em>)</li>\n<li>scipy       (Tested on <em>0.13.3</em>, <em>0.17.1</em>)</li>\n<li>skimage     (Minimum Required <em>0.13.1</em>)</li>\n<li>sklearn     (Tested on <em>0.14.1</em>, <em>0.19.1</em>)</li>\n<li>h5py        (Tested on <em>2.7.1</em>)</li>\n</ul>\n</li>\n<li>Docker (optional, recommended, tested on <em>17.12.0-ce</em>)\n<ul>\n<li><a href=\"https://docs.docker.com/install/linux/docker-ce/ubuntu/\">Official page for install instructions</a></li>\n</ul>\n</li>\n</ul>\n<h4>Download</h4>\n<ol>\n<li>In your workspace, clone the repositories:<pre><code>git clone https://github.com/oravus/lostX.git\ncd lostX\ngit clone https://github.com/oravus/refinenet.git\n</code></pre>\nNOTE: If you download this repository as a zip, the refineNet's fork will not get downloaded automatically, being a git submodule.</li>\n<li>Download the Resnet-101 model pre-trained on Cityscapes dataset from <a href=\"https://drive.google.com/drive/folders/1U2c1N6QJdzB_8HBgXb7mJ6Qk66JDBHI9\">here</a> or <a href=\"https://pan.baidu.com/s/1nxf2muP#list/path=%2Frefinenet_public_new%2Frefinenet_released%2Frefinenet_res101&amp;parentPath=%2Frefinenet_public_new%2Frefinenet_released\">here</a>. More details on RefineNet's <a href=\"https://github.com/guosheng/refinenet\">Github page</a>.\n<ul>\n<li>Place the downloaded model's <code>.mat</code> file in the <code>refinenet/model_trained/</code> directory.</li>\n</ul>\n</li>\n<li>If you are using docker, download the docker image:<pre><code>docker pull souravgarg/vpr-lost-kc:v1\n</code></pre>\n</li>\n</ol>\n<h4>Run</h4>\n<ol>\n<li>\n<p>Generate and store semantic labels and dense convolutional descriptors from RefineNet's <em>conv5</em> layer\nIn the MATLAB workspace, from the <code>refinenet/main/</code> directory, run:</p>\n<pre><code>demo_predict_mscale_cityscapes\n</code></pre>\n<p>The above will use the sample dataset from <code>refinenet/datasets/</code> directory. You can set path to your data in <code>demo_predict_mscale_cityscapes.m</code> through variable <code>datasetName</code> and <code>img_data_dir</code>.<br>\nYou might have to run <code>vl_compilenn</code> before running the demo, please refer to the instructions for running refinenet in their official <a href=\"https://github.com/guosheng/refinenet\">Readme.md</a></p>\n</li>\n<li>\n<p>[For Docker users]<br>\nIf you have an environment with python and other dependencies installed, skip this step, otherwise run a docker container:</p>\n<pre><code>docker run -it -v PATH_TO_YOUR_HOME_DIRECTORY/:/workspace/ souravgarg/vpr-lost-kc:v1 /bin/bash\n</code></pre>\n<p>From within the docker container, navigate to <code>lostX/lost_kc/</code> repository.<br>\n<code>-v</code> option mounts the <em>PATH_TO_YOUR_HOME_DIRECTORY</em> to <em>/workspace</em> directory within the docker container.</p>\n</li>\n<li>\n<p>Reformat and pre-process RefineNet's output from <code>lostX/lost_kc/</code> directory:</p>\n<pre><code>python reformat_data.py -p $PATH_TO_REFINENET_OUTPUT\n</code></pre>\n<p>$PATH_TO_REFINENET_OUTPUT is set to be the parent directory of <code>predict_result_full</code>, for example, <em>../refinenet/cache_data/test_examples_cityscapes/1-s_result_20180427152622_predict_custom_data/predict_result_1/</em></p>\n</li>\n<li>\n<p>Compute LoST descriptor:</p>\n<pre><code>python LoST.py -p $PATH_TO_REFINENET_OUTPUT \n</code></pre>\n</li>\n<li>\n<p>Repeat step 1, 3, and 4 to generate output for the other dataset by setting the variable <code>datasetName</code> to <code>2-s</code>.</p>\n</li>\n<li>\n<p>Perform place matching using LoST descriptors based difference matrix and Keypoint Correspondences:</p>\n<pre><code>python match_lost_kc.py -n 10 -f 0 -p1 $PATH_TO_REFINENET_OUTPUT_1  -p2 $PATH_TO_REFINENET_OUTPUT_2\n</code></pre>\n</li>\n</ol>\n<p>Note: Run <code>python FILENAME -h</code> for any of the python source files in Step 3, 4, and 6 for description of arguments passed to those files.</p>\n<h2>License</h2>\n<p>The code is released under MIT License.</p>\n<h2>Related Projects</h2>\n<p><a href=\"https://github.com/oravus/DeltaDescriptors\">Delta Descriptors (2020)</a></p>\n<p><a href=\"https://github.com/oravus/CoarseHash\">CoarseHash (2020)</a></p>\n<p><a href=\"https://github.com/oravus/seq2single\">seq2single (2019)</a></p>\n","name":"LoST-X","type":"code","url":"https://github.com/oravus/lostX","id":"lost_code","image":"lost_kc/bin/day-night-keypoint-correspondence-place-recognition.jpg","_images":["/_next/static/images/day-night-keypoint-correspondence-place-recognition-38203057bf036a1e9271b0a7647119fa.jpg.webp","/_next/static/images/day-night-keypoint-correspondence-place-recognition-bed6f778b7ec1ce4edaa346e24fb33bf.jpg"],"src":"/content/visual_place_recognition/lost.md","image_position":"center"}},"__N_SSG":true}