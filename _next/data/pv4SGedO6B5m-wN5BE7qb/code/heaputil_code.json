{"pageProps":{"codeData":{"content":"<h1>A Hierarchical Dual Model of Environment- and Place-Specific Utility for Visual Place Recognition</h1>\n<p><a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"><picture><img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg?style=flat-square\"></picture></a>\n<a href=\"https://github.com/Nik-V9/HEAPUtil/stargazers\"><picture><img alt=\"stars\" src=\"https://img.shields.io/github/stars/Nik-V9/HEAPUtil?style=social\"></picture></a>\n<a href=\"https://qcr.github.io/collection/vpr_overview/\"><picture><img alt=\"QUT Centre for Robotics\" src=\"https://img.shields.io/badge/collection-QUT%20Robotics-%23043d71?style=flat-square\"></picture></a>\n<a href=\"https://arxiv.org/abs/2107.02440\"><picture><img alt=\"arXiv\" src=\"https://img.shields.io/badge/arXiv-2107.02440-b31b1b.svg\"></picture></a>\n<a href=\"https://ieeexplore.ieee.org/abstract/document/9484750\"><picture><img alt=\"IEEE Xplore RA-L 2021\" src=\"https://img.shields.io/badge/-IEEE%20Xplore%20RA--L%202021-blue\"></picture></a>\n<a href=\"https://colab.research.google.com/github/Nik-V9/HEAPUtil/blob/main/HEAPUtil_Demo.ipynb\"><picture><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></picture></a>\n<a href=\"https://youtu.be/CbAkVsk0KYE\"><picture><img alt=\"YouTube\" src=\"https://img.shields.io/badge/YouTube-FF0000?style=flat&amp;logo=youtube&amp;logoColor=white\"></picture></a></p>\n<p><a href=\"https://paperswithcode.com/sota/visual-place-recognition-on-berlin-kudamm?p=a-hierarchical-dual-model-of-environment-and\"><picture><img alt=\"PWC\" src=\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a-hierarchical-dual-model-of-environment-and/visual-place-recognition-on-berlin-kudamm\"></picture></a></p>\n<h2>Introduction</h2>\n<p>HEAPUtil is an IEEE RA-L &amp; IROS 2021 research paper. In this work, we present a method for unsupervised estimation of the Environment-Specific (ES) and Place-Specific (PS) Utility of unique visual cues in a reference map represented as VLAD clusters. Furthermore, we employ this Utility in a unified hierarchical global-to-local VPR pipeline to enable better place recognition and localization capability for robots, with reduced storage and compute time requirements. This repo contains the official code for estimating the Utility of visual cues and the hierarchical global-to-local VPR pipeline.</p>\n<p align=\"center\">\n  <picture><img alt=\"\" src=\"https:/github.com/Nik-V9/HEAPUtil/raw/HEAD/assets/overview.jpg\"></picture>\n    <br><em>Utility-guided Hierarchical Visual Place Recognition.</em>\n</p>\n<p>For more details, please see:</p>\n<ul>\n<li>\n<p>Full paper PDF: <a href=\"https://arxiv.org/abs/2107.02440\">A Hierarchical Dual Model of Environment- and Place-Specific Utility for Visual Place Recognition</a>.</p>\n</li>\n<li>\n<p>Authors: <em>Nikhil Varma Keetha, Michael Milford, Sourav Garg</em></p>\n</li>\n</ul>\n<h2>Dependencies</h2>\n<p>Simply run the following command: <code>pip install -r requirements.txt</code></p>\n<h3>Conda</h3>\n<pre class=\"language-bash\"><code class=\"language-bash\">conda create -n heaputil <span class=\"token assign-left variable\">python</span><span class=\"token operator\">=</span><span class=\"token number\">3.8</span> mamba -c conda-forge -y\nconda activate heaputil\nmamba <span class=\"token function\">install</span> numpy opencv pytorch matplotlib faiss-gpu scipy scikit-image<span class=\"token operator\">=</span><span class=\"token number\">0.18</span>.2 torchvision scikit-learn h5py -c conda-forge\n</code></pre>\n<h2>Data</h2>\n<p>For Data Loading, we use <code>.mat</code> files which contain information regarding <code>Reference Image Paths</code>, <code>Query Image Paths</code>, <code>Ground-truth Co-ordinates</code> for Reference and Query Images, and the <code>Positive Localization Distance Threshold</code>. These <code>.mat</code> files for the Berlin Kudamm, Nordland Summer Vs Winter and Oxford Day Vs Night datasets are present in the <code>./dataset-mat-files</code> folder.</p>\n<p>We provide the Berlin Kudamm Dataset for Inference:</p>\n<ul>\n<li><a href=\"https://mega.nz/file/RppWTKQR#6ZiC_TICKy4NRGXo6b5n5IVIzn7usWtF0v-EmLczMns\">MEGA</a></li>\n</ul>\n<p>For more details regarding the Berlin Kudamm dataset please refer to <a href=\"https://arxiv.org/abs/2002.03895\">this paper</a>.</p>\n<p>For all the scripts, apart from SuperPoint Extraction, you may use the <code>--dataset</code> flag to mention the dataset to use. By default, it is set to <code>'berlin'</code> and the default choices are <code>['oxford', 'nordland', 'berlin']</code>.</p>\n<h2>Quick Start</h2>\n<p>Here's a <a href=\"https://colab.research.google.com/github/Nik-V9/HEAPUtil/blob/main/HEAPUtil_Demo.ipynb\">Colab Notebook</a> to effortlessly run tests on the Berlin Dataset.</p>\n<h2>Scripts</h2>\n<p>Please use the <code>--help</code> flag to see all available arguments for the scripts.</p>\n<h3>NetVLAD (Global Descriptor)</h3>\n<p>Extract NetVLAD Descriptors, Predictions and Cluster Masks:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python NetVLAD/main.py --resume './data/NetVLAD/netvlad-checkpoint-cc16' --root_dir './data' --save --save_path './data/NetVLAD'\n</code></pre>\n<h3>Environment- and Place-Specific Utility Estimation</h3>\n<p>Estimate the Environment- and Place-Specific Utility of VLAD Clusters for the Reference Map:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python utility.py --root_dir './data' --netvlad_extracts_path './data/NetVLAD' --save_path './data/Utility' --save_viz\n</code></pre>\n<p>You may use the <code>--save_viz</code> flag to visualize the Environment-Specific and Place-Specific Utility as shown below:</p>\n<p align=\"center\">\n  <video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/es_utility-b08d6e5a8939892bf3756d486f43bb6b.webp\"><source src=\"/_next/static/images/es_utility-b08d6e5a8939892bf3756d486f43bb6b.webm\" type=\"video/webm\"><source src=\"/_next/static/images/es_utility-b08d6e5a8939892bf3756d486f43bb6b.mp4\" type=\"video/mp4\"></video>  &nbsp; &nbsp; <video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/ps_utility-05d2dd2e2211144131b4181279fc6025.webp\"><source src=\"/_next/static/images/ps_utility-05d2dd2e2211144131b4181279fc6025.webm\" type=\"video/webm\"><source src=\"/_next/static/images/ps_utility-05d2dd2e2211144131b4181279fc6025.mp4\" type=\"video/mp4\"></video>\n    <br><em> Visualizing ES (left) &amp; PS (right) Utility (Red indicates low utility and blue/gray indicates high utility)</em>\n</p>\n<h3>SuperPoint Feature Extraction</h3>\n<p>Generate path lists which are required for SuperPoint Extraction &amp; SuperGlue:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python generate_path_lists.py --root_dir './data' --netvlad_predictions './data/NetVLAD' --save_path './data'\n</code></pre>\n<p>Extract SuperPoint features for the Reference Map:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python SuperGlue/superpoint_extraction.py --input_images './data/db_list.txt' --split 'db' --input_dir './data' --output_dir './data/SuperPoint'\n</code></pre>\n<p>Extract SuperPoint features for the Queries:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python SuperGlue/superpoint_extraction.py --input_images './data/q_list.txt' --split 'query' --input_dir './data' --output_dir './data/SuperPoint'\n</code></pre>\n<h3>Utility-guided Local Feature Matching</h3>\n<p>You may use the <code>--viz</code> flag to visualize the best matches as a gif.</p>\n<h4>Vanilla</h4>\n<p>Run Vanilla SuperPoint based Local Feature Matching:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python local_feature_matching.py --input_dir './data' --output_dir './data/LFM/Vanilla' \\\n--netvlad_extracts_path './data/NetVLAD' --superpoint_extracts_path './data/SuperPoint' --utility_path './data/Utility'\n</code></pre>\n<h4>Environment-Specific (ES) Utility</h4>\n<p>Run ES-Utility guided Local Feature Matching:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python local_feature_matching.py --input_dir './data' --output_dir './data/LFM/ES_Utility' \\\n--netvlad_extracts_path './data/NetVLAD' --superpoint_extracts_path './data/SuperPoint' --utility_path './data/Utility' \\\n--es_utility\n</code></pre>\n<h4>Place-Specific (PS) Utility</h4>\n<p>Run PS-Utility guided Local Feature Matching:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python local_feature_matching.py --input_dir './data' --output_dir './data/LFM/PS_Utility' \\\n--netvlad_extracts_path './data/NetVLAD' --superpoint_extracts_path './data/SuperPoint' --utility_path './data/Utility' \\\n--ps_utility\n</code></pre>\n<p>Default Number of Top Utility Clusters to use for Local Feature Matching is <code>10</code>. Please use the <code>--k</code> flag to use a different number of top utility clusters.</p>\n<h4>Combined ES &amp; PS Utility</h4>\n<p>Run ES &amp; PS-Utility guided Local Feature Matching:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python local_feature_matching.py --input_dir './data' --output_dir './data/LFM/Utility' \\\n--netvlad_extracts_path './data/NetVLAD' --superpoint_extracts_path './data/SuperPoint' --utility_path './data/Utility' \\\n--es_utility --ps_utility --viz\n</code></pre>\n<p>Default Number of Top Utility Clusters to use for Local Feature Matching is <code>X-1</code> clusters, where <code>X</code> is the number of useful clusters determined by the Environment-Specific system. To use a different number of top utility clusters please use the <code>--non_default_k</code> and <code>--k</code> flags.</p>\n<p>We use the <code>--viz</code> flag to visualize the best matches along with utility reference masks as a gif as shown below:</p>\n<p align=\"center\">\n  <video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/matches-09e725c71a8cb66d2a0fac1b0373bc00.webp\"><source src=\"/_next/static/images/matches-09e725c71a8cb66d2a0fac1b0373bc00.webm\" type=\"video/webm\"><source src=\"/_next/static/images/matches-09e725c71a8cb66d2a0fac1b0373bc00.mp4\" type=\"video/mp4\"></video>\n    <br><em>ES &amp; PS Utility-guided Local Feature Matching (Cyan mask represents regions with high utility)</em>\n</p>\n<h3>Utility-guided SuperGlue</h3>\n<p>Similar to Local Feature Matching, you may run the <code>superglue_match_pairs.py</code> file for Vanilla SuperGlue &amp; Utility-guided SuperGlue. You may use the <code>--viz</code> flag to visualize all the matches and dump the SuperGlue-style plots.</p>\n<p>Run ES &amp; PS-Utility guided SuperGlue:</p>\n<pre class=\"language-sh\"><code class=\"language-sh\">python superglue_match_pairs.py --input_pairs './data/berlin_netvlad_candidate_list.txt' --input_dir './data' --output_dir './data/SuperGlue/Utility' \\\n--netvlad_extracts_path './data/NetVLAD' --utility_path './data/Utility' \\\n--es_utility --ps_utility\n</code></pre>\n<h2>BibTeX Citation</h2>\n<p>If any ideas from the paper or code from this repo are used, please consider citing:</p>\n<pre class=\"language-txt\"><code class=\"language-txt\">@article{keetha2021hierarchical,\n  author={Keetha, Nikhil Varma and Milford, Michael and Garg, Sourav},\n  journal={IEEE Robotics and Automation Letters}, \n  title={A Hierarchical Dual Model of Environment- and Place-Specific Utility for Visual Place Recognition}, \n  year={2021},\n  volume={6},\n  number={4},\n  pages={6969-6976},\n  doi={10.1109/LRA.2021.3096751}}\n</code></pre>\n<p>The code is licensed under the <a href=\"./LICENSE\">MIT License</a>.</p>\n<h2>Acknowledgements</h2>\n<p>The authors acknowledge the support from the Queensland University of Technology (QUT) through the Centre for Robotics.</p>\n<p>Furthermore, we would like to acknowledge the <a href=\"https://github.com/Nanne/pytorch-NetVlad\">Pytorch Implementation of NetVlad</a> from Nanne and the original implementation of <a href=\"https://github.com/magicleap/SuperGluePretrainedNetwork\">SuperGlue</a>.</p>\n<h2>Related works</h2>\n<p>Please check out <a href=\"https://qcr.github.io/collection/vpr_overview/\">this collection</a> of related works on place recognition.</p>\n","name":"HEAPUtil","type":"code","url":"https://github.com/Nik-V9/HEAPUtil","id":"heaputil_code","image":"assets/overview.jpg","_images":["/_next/static/images/overview-8c193585e23714439d55f0227d88f923.jpg.webp","/_next/static/images/overview-fc609d6102a3c08cb20b14382e57ee50.jpg"],"src":"/content/visual_place_recognition/heaputil.md","image_position":"center"}},"__N_SSG":true}