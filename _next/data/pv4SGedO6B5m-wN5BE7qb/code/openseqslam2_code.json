{"pageProps":{"codeData":{"content":"<h1>OpenSeqSLAM2.0 Toolbox</h1>\n<p><picture><img alt=\"The various interactive screens in OpenSeqSLAM2\" src=\"https:/github.com/qcr/openseqslam2/raw/HEAD/docs/openseqslam2.png\"></picture></p>\n<p>OpenSeqSLAM2.0 is a MATLAB toolbox that allows users to thoroughly explore the SeqSLAM method in addressing the visual place recognition problem. The visual place recognition problem is centred around recognising a previously traversed route, regardless of whether it is seen during the day or night, in clear or inclement conditions, or in summer or winter. Recognising previously traversed routes is a crucial capability of navigating robots. Through the graphical interfaces packaged in OpenSeqSLAM2 users are able to:</p>\n<ul>\n<li>explore a number of previously published variations to the SeqSLAM method (including search and match selection methods);</li>\n<li>visually track progress;</li>\n<li>interactively tune parameters;</li>\n<li>dynamically reconfigure matching parameters while viewing results;</li>\n<li>explore precision-recall statistics;</li>\n<li>visualise difference matrices, match sequence images, and image pre-processing steps;</li>\n<li>view and export matching videos;</li>\n<li>automatically optimise selection thresholds against a ground truth;</li>\n<li>sweep any numeric parameter value through a batch operation mode; and</li>\n<li>operate in headless mode with parallelisation available.</li>\n</ul>\n<p>The toolbox is open-source and downloadable from the <a href=\"https://github.com/qcr/openseqslam2/releases\">releases tab</a>. All we ask is that if you use OpenSeqSLAM2 in any academic work, that you include a reference to corresponding publication (bibtex is available at the bottom of the page).</p>\n<h2>How to use the toolbox</h2>\n<p>The toolbox is designed to be simple to use (it runs out of the box without any initial configuration required). To run the toolbox, simple run the command below (with the toolbox root directory in your MATLAB path):</p>\n<pre class=\"language-matlab\"><code class=\"language-matlab\"><span class=\"token function\">OpenSeqSLAM2</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre>\n<p>There are a number of default configuration files included in the <code>.config</code> directory which showcase the capabilities of the toolbox. To use a configuration file, open the toolbox as described above, then use the <code>Import config</code> button. A summary of the features showcased in each of the configuration files is included below:</p>\n<ul>\n<li><strong><code>'images_same'</code></strong>: The trimmed Nordland dataset images, with the same dataset used as both reference and query. Trajectory based search is used, and a velocity-based ground truth is included, but not used for auto-optimisation of match threshold.</li>\n<li><strong><code>'images_diff'</code></strong>: The trimmed Nordland dataset images, with the summer traversal used as the reference dataset and the winter traversal as the query. Trajectory based search is used, and a *.csv based ground truth is used for auto-optimising the match threshold selection.</li>\n<li><strong><code>'videos_same'</code></strong>: The day night video dataset, with the same video used as both the reference and query dataset. Trajectory based search is used, with no ground truth provided.</li>\n<li><strong><code>'videos_diff'</code></strong>: The day night video dataset, with the day traversal used as the reference dataset and the night traversal as the query. Trajectory based search is used, with no ground truth provided.</li>\n<li><strong><code>'hybrid_search'</code></strong>: Same as <code>'videos_diff'</code>, but the hybrid search is used instead of trajectory search.</li>\n<li><strong><code>'no_gui'</code></strong>: Same as <code>'videos_diff'</code>, but the progress is presented in the console rather than GUI and no results GUI is shown (tip: run OpenSeqSLAM2(‘<configpath>/no_gui.xml’) to see how the toolbox can run entirely headless)</configpath></li>\n<li><strong><code>'batch_with_gui'</code></strong>: Same as <code>'images_diff'</code>, but a batch parameter sweep of the sequence length parameter is performed. The progress GUI shows the progress of the individual iteration and overall in separate windows.</li>\n<li><strong><code>'parrallelised_batch'</code></strong>: Same as <code>'batch_with_gui'</code>, but the parameter sweep is done in parallel mode (which cannot be performed with the Progress GUI). The parallel mode will use a worker for each core available in the host CPU.</li>\n<li><strong><code>'default'</code></strong>: is set to <code>'images_diff'</code></li>\n</ul>\n<p><em><em>Note:</em> the programs in the <code>./bin</code> directory can be run standalone by providing the appropriate results / config structs as arguments if you would like to use only a specific part of the pipeline (i.e. only configuration, or progress wrapped execution, or viewing results).</em></p>\n<h2>Citation details</h2>\n<p>If using the toolbox in any academic work, please include the following citation:</p>\n<pre class=\"language-bibtex\"><code class=\"language-bibtex\">@ARTICLE{2018openseqslam2,\n   author = {{Talbot}, B. and {Garg}, S. and {Milford}, M.},\n    title = \"{OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions}\",\n  journal = {ArXiv e-prints},\narchivePrefix = \"arXiv\",\n   eprint = {1804.02156},\n primaryClass = \"cs.RO\",\n keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},\n     year = 2018,\n    month = apr,\n   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402156T},\n  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\n</code></pre>\n","name":"OpenSeqSLAM2","type":"code","url":"https://github.com/qcr/openseqslam2","id":"openseqslam2_code","image":"./docs/openseqslam2.png","_images":["/_next/static/images/openseqslam2-c5079d59d4cff5bd652acb1652d047f6.png.webp","/_next/static/images/openseqslam2-f3755fc8e61c0d81c8f0b0f42c5e08ae.png"],"src":"/content/visual_place_recognition/openseqslam2.md","image_position":"center"}},"__N_SSG":true}