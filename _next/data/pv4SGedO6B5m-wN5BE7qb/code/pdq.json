{"pageProps":{"codeData":{"content":"<p><a href=\"https://qcr.github.io\"><picture><img alt=\"QUT Centre for Robotics Open Source\" src=\"https://github.com/qcr/qcr.github.io/raw/master/misc/badge.svg\"></picture></a></p>\n<h1>Probability-based Detection Quality (PDQ)</h1>\n<p>This repository contains the implementation of the probability-based detection quality (PDQ) evaluation measure.\nThis enables <strong>quantitative</strong> analysis of the <strong>spatial and semantic uncertainties</strong> output by a probabilistic object detecttion (PrOD) system.\nThis repository provides tools for analysing PrOD detections and classical detections using mAP, moLRP, and PDQ (note that PDQ results will be low for a classical detector and mAP and moLRP scores will likely be low for PrOD detections).\nEvaluation can be performed both on COCO formatted data and on RVC1 (PrOD challenge) formatted data.\nThe repository also provides visualization tools to enable fine-grained analysis of PDQ results as shown below.</p>\n<p><picture><img alt=\"PrOD evaluation visualization image examples\" src=\"https:/github.com/david2611/pdq_evaluation/raw/HEAD/docs/PDQ_Examples.jpg\"></picture></p>\n<p>The code here, particularly for evaluating RVC1 data is based heavily on the PrOD challenge code which can be found\nhere: https://github.com/jskinn/rvchallenge-evaluation</p>\n<p>Note that some extra funcitonality for PDQ outside of what is reported in the original paper and challenge is also provided such as evaluating results using the bounding boxes of the ground-truth segmentation masks, probabilistic segmentation evaluation, a greedy alternative to PDQ.</p>\n<p>For further details on the robotic vision challenges please see the following links for more details:</p>\n<ul>\n<li>Robotic Vision Challenges Homepage: http://roboticvisionchallenge.org/</li>\n<li>PrOD Main Challenge Page: https://competitions.codalab.org/competitions/20597</li>\n<li>PrOD Continuous Challenge Page: https://competitions.codalab.org/competitions/20595</li>\n</ul>\n<h1>Citing PDQ</h1>\n<p>If you are using PDQ in your research, please cite the paper below:</p>\n<pre><code>@inproceedings{hall2020probabilistic,\n  title={Probabilistic object detection: Definition and evaluation},\n  author={Hall, David and Dayoub, Feras and Skinner, John and Zhang, Haoyang and Miller, Dimity and Corke, Peter and Carneiro, Gustavo and Angelova, Anelia and S{\\\"u}nderhauf, Niko},\n  booktitle={The IEEE Winter Conference on Applications of Computer Vision},\n  pages={1031--1040},\n  year={2020}\n}\n</code></pre>\n<h1>Setup</h1>\n<h2>Install all python requirements</h2>\n<p>This code comes with a requirements.txt file.\nMake sure you have installed all libraries as part of your working environment.</p>\n<h2>Install COCO mAP API</h2>\n<p>After installing all requirements, you will need to have a fully installed implementation of the COCO API located\nsomewhere on your machine.\nYou can download this API here https://github.com/cocodataset/cocoapi.</p>\n<p>Once this is downloaded and installed, you need to adjust the system path on line 11 of coco_mAP.py and line 16 of\nread_files.py to match the PythonAPI folder of your COCO API installation.</p>\n<h2>Add LRP Evaluation Code</h2>\n<p>You will also require code for using LRP evaluation measures.\nTo do this you need to simply copy the cocoevalLRP.py file from the LRP github repository to the pycocotools folder within the PythonAPI.\nYou can download the specific file here https://github.com/cancam/LRP/blob/master/cocoLRPapi-master/PythonAPI/pycocotools/cocoevalLRP.py\nYou can clone the original repository here https://github.com/cancam/LRP.</p>\n<p>After cocoevalLRP.py is located in your pycocotools folder, simply adjust the system path on line 11 of coco_LRP.py to match your PythonAPI folder.</p>\n<h1>Usage</h1>\n<p>All evaluation code is run on detections saved in .json files formatted as required by the RVC outlined later on.\nA variation to this is also available for probabilistic segmentation format also described later.\nIf you are evaluating on COCO data and have saved detections in COCO format, you can convert to RVC1 format using\n<em>file_convert-coco_to_rvc1.py</em>\nWhen you have the appropriate files, you can evaluate on mAP, moLRP, and PDQ with <em>evaluate.py</em>.\nAfter evaluation is complete, you can visualise your detections for a sequence of images w.r.t. PDQ using\n<em>visualise_pdq_analysis.py</em></p>\n<p>Evaluation is currently organised so that you can evaluate either on COCO data, or on RVC1 data. Note that RVC1 data\nexpects multiple sequences rather than a single folder of data.</p>\n<h2>RVC1 Detection Format</h2>\n<p>RVC1 detections are saved in a single .json file per sequence being evaluated. Each .json file is formatted as follows:</p>\n<pre><code>{\n  \"classes\": [&lt;an ordered list of class names&gt;],\n  \"detections\": [\n    [\n      {\n        \"bbox\": [x1, y1, x2, y2],\n        \"covars\": [\n          [[xx1, xy1],[xy1, yy1]],\n          [[xx2, xy2],[xy2, yy2]]\n        ],\n        \"label_probs\": [&lt;an ordered list of probabilities for each class&gt;]\n      },\n      {\n      }\n    ],\n    [],\n    []\n    ...\n  ]\n}\n</code></pre>\n<h3>Important Notes</h3>\n<p>The two covariance matrices in <code>covars</code> need to be positive semi-definite in order for the code to work. A covariance matrix <code>C</code> is positive semi-definite when its eigenvalues are not negative. You can easily check this condition in python with the following function:</p>\n<pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">is_pos_semidefinite</span><span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> np<span class=\"token punctuation\">.</span><span class=\"token builtin\">all</span><span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>linalg<span class=\"token punctuation\">.</span>eigvals<span class=\"token punctuation\">(</span>C<span class=\"token punctuation\">)</span> <span class=\"token operator\">&gt;=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n</code></pre>\n<h3>Probabilistic Segmentation Detections</h3>\n<p>We now accommodate a way to submit probabilistic segmentation detections.\nFor this format, a .npy file for each image stores all detection probabilistic segmentation heatmaps for that image.\nThis 3D array's shape is m x h x w where m is the number of segmentation masks, h is the image height, and w is the\nimage width.\nEach detection dictionary now contains the location the .npy file associated with the detection and the mask id for the\nspecific detection.\nYou may also define a bounding box to replace the probabilistic segmentation for bounding-box detections and define a\nchosen class to use for mAP and moLRP evaluation (rather than always using max class of label_probs).</p>\n<p>Expected format for probabilistic segmentation detection files is as follows:</p>\n<pre><code>{\n  \"classes\": [&lt;an ordered list of class names&gt;],\n  \"detections\": [\n    [\n      {\n        \"label_probs\": [&lt;an ordered list of probabilities for each class&gt;],\n        \"masks_file\": \"&lt;location of .npy file holding probabilistic segmentation mask&gt;\",\n        \"mask_id\": &lt;index of this detection's mask in mask_file's numpy array&gt;,\n        \"label\": &lt;chosen label within label_probs&gt; (optional),\n        \"bbox\": [x1, y1, x2, y2] (optional for use in mAP and moLRP),\n      },\n      {\n      }\n    ],\n    [],\n    []\n    ...\n  ]\n}\n</code></pre>\n<h2>file_convert_coco_to_rvc1.py</h2>\n<p>To convert coco detections to rvc format simply run:</p>\n<p><code>python file_convert_coco_to_rvc1.py --coco_gt &lt;gt_json_file&gt; --coco_det &lt;det_json_file&gt; --rvc1_det &lt;output_json_file&gt;</code></p>\n<p>where <code>&lt;gt_json_file&gt;</code> is the coco format ground-truth json filename, <code>det_json_file</code> is the coco format detection\njson filename, and <code>output_file</code> is the json filename you will save your rvc1 formatted detections json file.</p>\n<h3>Important Notes</h3>\n<p>By default, coco json format does not come with the predicted scores for all the classes available, in which case the conversion script will just\nextract the score of the chosen class and distribute remaining probability across all others classes. However, this will produce\nincorrect measures of label quality because it is the probability estimated by the detector for the object's ground-truth class, which might not\ncorrespond to the chosen class. To facilitate correct measurements, if a detection element in the coco json file (<code>det_json_file</code>) comes with a\nkey <code>all_scores</code>, the conversion script will consider it as an array of all the scores, and use it instead of the default behaviour.</p>\n<p>Also, by default, coco json format does not consider the existence of a covariance matrix which is needed for PDQ calculations. The conversion\nscript assigns by default a zero'ed covariance matrix, but if a detection element in the coco json file (<code>det_json_file</code>) comes with a\nkey <code>covars</code>, the conversion script will use that covariance matrix instead of the default one with zeros. Please refer to the previous section <code>RVC1 Detection Format</code> for further information on how <code>covars</code> should be formatted in the json file.</p>\n<h2>evaluate.py</h2>\n<p>To perform full evaluation simply run:</p>\n<p><code>python evaluate.py --test_set &lt;test_type&gt; --gt_loc &lt;gt_location&gt; --det_loc &lt;det_location&gt; --save_folder &lt;save_folder&gt; --set_cov &lt;cov&gt; --num_workers &lt;num_workers&gt;</code></p>\n<p>Optional flags for new functionality include <code>--bbox_gt</code>, <code>--segment_mode</code>, <code>--greedy_mode</code>, and <code>--prob_seg</code>.\nThere is also an <code>--mAP_heatmap</code> flag but that should not generally be used.</p>\n<ul>\n<li>\n<p><code>&lt;test_type&gt;</code> is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'</p>\n</li>\n<li>\n<p><code>&lt;gt_location&gt;</code> is a string defining either the location of a ground-truth .json file (coco tests) or a folder of\nground truth sequences (rvc1 data). Which one it is interpreted as is defined by <code>&lt;test_type&gt;</code></p>\n</li>\n<li>\n<p><code>&lt;det_loc&gt;</code> is a string defining either the location of a detection .json file (coco data) or a folder of .json files for\nmultiple sequences (rvc1 data). Which one it is interpreted as is defined by <code>&lt;test_type&gt;</code>.\nNote that these detection files must be in rvc1 format.</p>\n</li>\n<li>\n<p><code>&lt;save_folder&gt;</code> is a string defining the folder where analysis will be stored in form of scores.txt, and files for visualisations</p>\n</li>\n<li>\n<p><code>&lt;cov&gt;</code> is an optional value defining set covariance for the corners of detections.</p>\n</li>\n<li>\n<p><code>--bbox_gt</code> flag states that all ground-truth should be teated as bounding boxes for PDQ analysis.\nAll pixels within the bounding box will be used for analysis and there will be no \"ignored\" pixels. This enables\nuse of datasets with no segmentation information provided they are stored in COCO ground-truth format.</p>\n</li>\n<li>\n<p><code>--segment_mode</code> flag states that evaluation is performed per-pixel on the ground-truth segments with no \"ignored\"\npixels to accommodate box-shaped detections. This should only be used if evaluating a probabilistic segmentation\ndetection system.</p>\n</li>\n<li>\n<p><code>--greedy_mode</code> flag states that assignment of detections to ground-truth objects based upon pPDQ scores is done\ngreedily rather than optimal assignment. Greedy mode can be faster for some applications but does not match \"official\"\nPDQ process and there may be some minuscule difference in score/behaviour.</p>\n</li>\n<li>\n<p><code>--prob_seg</code> flag states that detection.json file is formatted for probabilistic segmentation detections as outlined\nabove.</p>\n</li>\n<li>\n<p><code>--mAP_heatmap</code> flag should not generally be used but enables mAP/moLRP evaluation to be based not upon corners\ndefined by PBox/BBox detections, but that encompass all pixels of the detection above given threshold of probability\n(0.0027).</p>\n</li>\n<li>\n<p><code>--num_workers</code> number of parallel worker processes to use in the CPU when making the calculations for the PDQ score. By default, this value is 6.</p>\n</li>\n</ul>\n<p>For further details, please consult the code.</p>\n<h3>Important Notes</h3>\n<p>For consistency reasons, unlike the original rvc1 evaluation code, we do not multiply PDQ by 100 to provide it as a percentage.\nPDQ is also labelled as \"PDQ\" in scores.txt rather than simply \"score\".</p>\n<p>For anyone unfamiliar with moLRP based measures, these values are losses and not qualities like all other provided measures.\nTo transform these results from losses to qualities simply take 1 - moLRP.</p>\n<p>Newly implemented modes <code>--segment_mode</code>, <code>--bbox_gt</code>, <code>greedy_mode</code> are not used for the RVC1 challenge but can be\nuseful for developing research in probabilistic segmentation, when your dataset does not have a segmentation mask, or\nwhen time is critical, respectively.</p>\n<h2>visualise_pdq_analysis.py</h2>\n<p>To create visualisations for probabilistic detections and PDQ analysis on a single sequence of images run:</p>\n<p><code>python visualise_pdq_analysis.py --data_type &lt;test_type&gt; --ground_truth &lt;gt_location&gt; --gt_img_folder &lt;gt_imgs_location&gt; --det_json &lt;det_json_file&gt; --gt_analysis &lt;gt_analysis_file&gt; --det_analysis &lt;det_analysis_file&gt; --save_folder &lt;save_folder_location&gt; --set_cov &lt;cov&gt; --img_type &lt;ext&gt; --colour_mode &lt;colour_mode&gt; --corner_mode &lt;corner_mode&gt; --img_set &lt;list_of_img_names&gt; --full_info</code></p>\n<p>where:</p>\n<ul>\n<li>\n<p><code>&lt;test_type&gt;</code> is a string defining whether we are evaluating COCO or RVC1 data. Options are 'coco' and 'rvc1'</p>\n</li>\n<li>\n<p><code>&lt;gt_location&gt;</code> is a string defining either the location of a ground-truth .json file (coco tests) or a folder of\nground truth sequences (rvc1 data). Which one it is interpreted as is defined by <code>&lt;test_type&gt;</code></p>\n</li>\n<li>\n<p><code>&lt;gt_imgs_location&gt;</code> a string defining the folder where ground-truth images for the sequence are stored.</p>\n</li>\n<li>\n<p><code>&lt;det_json_file&gt;</code> a string defining the detection .json file matching the sequence to be visualised</p>\n</li>\n<li>\n<p><code>&lt;gt_analysis&gt;</code> a string defining the ground-truth analysis .json file matching the sequence to be visualised.\nMust also correspond to the detection .json file being visualised.</p>\n</li>\n<li>\n<p><code>&lt;det_analysis&gt;</code> a string defining the detection analysis .json file matching the sequence to be visualised.\nMust also correspond to the detection .json file being visualised.</p>\n</li>\n<li>\n<p><code>&lt;save_folder_location&gt;</code> a string defining the folder where image visualisations will be saved. Must be different to the <code>&lt;gt_imgs_location&gt;</code></p>\n</li>\n<li>\n<p><code>&lt;cov&gt;</code> is an optional value defining set covariance for the corners of the detections. <strong>This must match the set covariance used in evaluate.py</strong></p>\n</li>\n<li>\n<p><code>&lt;img_type&gt;</code> is a string defining what image type the ground-truth is provided in. For example 'jpg'.</p>\n</li>\n<li>\n<p><code>&lt;colour_mode&gt;</code> is a string defining whether correct and incorrect results are coloured green and red ('gr') or blue and orange ('bo') respectively.\nDefault option is blue and orange.</p>\n</li>\n<li>\n<p><code>&lt;corner_mode&gt;</code> is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').\nEllipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.\nArrows show 2 x standard deviation along the major axes of the Gaussian.\nDefault option is 'ellipse'</p>\n</li>\n<li>\n<p><code>&lt;list_of_img_names&gt;</code> is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.\nFor example <code>--img_set cat.jpg dog.jpg whale.jpg</code> would only draw visualisations for \"cat.jpg\", \"dog.jpg\", and \"whale.jpg\".</p>\n</li>\n<li>\n<p><code>--full_info</code> is an optional flag defining whether full pairwise quality analysis should be written for TP detections. <strong>Recommended setting for in-depth analysis</strong></p>\n</li>\n</ul>\n<p>For further details, please consult the code.</p>\n<h3>Important Notes</h3>\n<p>Consistency must be kept between ground-truth analysis, detection analysis, and detection .json files in order to provide meaningful visualisation.</p>\n<p>If the evaluation which produced the ground-truth analysis and detection analysis used a set covariance input, you must\nprovide that same set covariance when generating visualisations.</p>\n<p>New modes such as using probabilistic segmentation detections (<code>--prob_seg</code>) in segment mode (<code>--segment_mode</code>)\nor using bounding_box ground-truth (<code>--bbox_gt</code>) in the evaluation code are <strong>NOT</strong> yet supported.</p>\n<h2>visualise_prob_detections.py</h2>\n<p>To create visualisations for probabilistic detections on a single sequence of images run:</p>\n<p><code>python visualise_prob_detections.py --gt_img_folder &lt;gt_imgs_location&gt; --det_json &lt;det_json_file&gt; --save_folder &lt;save_folder_location&gt; --set_cov &lt;cov&gt; --img_type &lt;ext&gt; --corner_mode &lt;corner_mode&gt; --img_set &lt;list_of_img_names&gt;</code></p>\n<p>where:</p>\n<ul>\n<li>\n<p><code>&lt;gt_imgs_location&gt;</code> a string defining the folder where ground-truth images for the sequence are stored.</p>\n</li>\n<li>\n<p><code>&lt;det_json_file&gt;</code> a string defining the detection .json file matching the sequence to be visualised</p>\n</li>\n<li>\n<p><code>&lt;save_folder_location&gt;</code> a string defining the folder where image visualisations will be saved. Must be different to the <code>&lt;gt_imgs_location&gt;</code></p>\n</li>\n<li>\n<p><code>&lt;cov&gt;</code> is an optional value defining set covariance for the corners of the detections.</p>\n</li>\n<li>\n<p><code>&lt;img_type&gt;</code> is a string defining what image type the ground-truth is provided in. For example 'jpg'.</p>\n</li>\n<li>\n<p><code>&lt;corner_mode&gt;</code> is a string defining whether Gaussian corners are represented as three ellipses ('ellipse') or two arrows ('arrow').\nEllipses are drawn showing 1, 2, and 3, std deviation rings along the contours of the Gaussian.\nArrows show 2 x standard deviation along the major axes of the Gaussian.\nDefault option is 'ellipse'</p>\n</li>\n<li>\n<p><code>&lt;list_of_img_names&gt;</code> is an optional parameter where the user provides a set of image names and only these images will have visualisations drawn for them.\nFor example <code>--img_set cat.jpg dog.jpg whale.jpg</code> would only draw visualisations for \"cat.jpg\", \"dog.jpg\", and \"whale.jpg\".</p>\n</li>\n</ul>\n<p>For further details, please consult the code.</p>\n<h3>Important Notes</h3>\n<p>Order of detections in detections.json file must match the order of the images as stored in the ground-truth images\nfolder.</p>\n<p>New modes such as using probabilistic segmentation detections (<code>--prob_seg</code>) in the evaluation code are\n<strong>NOT</strong> yet supported.</p>\n<h1>Acknowledgements</h1>\n<p>Development of the probability-based detection quality evaluation measure was directly supported by:</p>\n<p><picture><img alt=\"Australian Centre for Robotic Vision\" src=\"https:/github.com/david2611/pdq_evaluation/raw/HEAD/docs/acrv_logo_small.png\"></picture></p>\n","name":"Probability-based Detection Quality (PDQ)","type":"code","url":"https://github.com/david2611/pdq_evaluation","image":"repo:/docs/qcr_web_img.jpg","image_fit":"contain","_images":["/_next/static/images/qcr_web_img-c5a515adb03792ab295e52f405822b65.jpg.webp","/_next/static/images/qcr_web_img-8b73fea58e143ca4e51ab20579b08efa.jpg"],"src":"/content/pdq.md","id":"pdq","image_position":"center"}},"__N_SSG":true}