{"pageProps":{"codeData":{"content":"<h2>Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation</h2>\n<p>This is the source code for the paper titled: \"Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation\", [<a href=\"https://arxiv.org/abs/1902.07381\">arXiv</a>][<a href=\"https://ieeexplore.ieee.org/abstract/document/8794178/\">IEEE Xplore</a>].</p>\n<p>If you find this work useful, please cite it as:\nGarg, S., Babu V, M., Dharmasiri, T., Hausler, S., Suenderhauf, N., Kumar, S., Drummond, T., &amp; Milford, M. (2019). Look no deeper: Recognizing places from opposing viewpoints under varying scene appearance using single-view depth estimation. In IEEE International Conference on Robotics and Automation (ICRA), 2019. IEEE.</p>\n<p>bibtex:</p>\n<pre><code>@inproceedings{garg2019look,\ntitle={Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation},\nauthor={Garg, Sourav and Babu V, Madhu and Dharmasiri, Thanuja and Hausler, Stephen and Suenderhauf, Niko and Kumar, Swagat and Drummond, Tom and Milford, Michael},\nbooktitle={IEEE International Conference on Robotics and Automation (ICRA)},\nyear={2019}\n}\n</code></pre>\n<p><picture><img alt=\"Illustration of the proposed approach\" src=\"https:/github.com/oravus/seq2single/raw/HEAD/gitPics/illustration.png\"></picture></p>\n<p><picture><img alt=\"An image depicting topometric representation.\" src=\"https:/github.com/oravus/seq2single/raw/HEAD/gitPics/topometric.png\"></picture></p>\n<h4>Requirements</h4>\n<ul>\n<li>Ubuntu\t(Tested on <em>16.04</em>)</li>\n<li>Jupyter\t(Tested on <em>4.4.0</em>)</li>\n<li>Python\t(Tested on <em>3.5.6</em>)\n<ul>\n<li>numpy\t(Tested on <em>1.15.2</em>)</li>\n<li>scipy\t(Tested on <em>1.1.0</em>)</li>\n</ul>\n</li>\n</ul>\n<p>Optionally, for vis_results.ipynb:</p>\n<ul>\n<li>Matplotlib\t(Tested on <em>2.0.2</em>)</li>\n</ul>\n<h4>Download an example dataset and its pre-computed representations</h4>\n<ol>\n<li>\n<p>In <code>seq2single/precomputed/</code>, download <a href=\"https://mega.nz/#F!Z4Z3gAzb!KI48uGHJJza90DP7-Kz1kA\">pre-computed representations (<em>~10 GB</em>)</a>. Please refer to the <code>seq2single/precomputed/readme.md</code> for instructions on how to compute these representations.</p>\n</li>\n<li>\n<p>[Optional] In <code>seq2single/images/</code>, download <a href=\"https://mega.nz/#F!h5QB2ayI!H7p0UCxATd6MUdszMZWNOA\">images (<em>~1 GB</em>)</a>. These images are a subset of two different traverses from the <a href=\"https://robotcar-dataset.robots.ox.ac.uk/\">Oxford Robotcar dataset</a>.</p>\n</li>\n</ol>\n<p>(Note: These download links from Mega.nz require you to first create an account (free))</p>\n<h4>Run</h4>\n<ol>\n<li>The Jupyter notebook seq2single.ipynb first loads the pre-computed global image descriptors to find top matches. These matches are re-ranked with the proposed method using the pre-computed depth masks and dense conv5 features.</li>\n</ol>\n<h4>License</h4>\n<p>The code is released under MIT License.</p>\n<h2>Related Projects</h2>\n<p><a href=\"https://github.com/oravus/DeltaDescriptors\">Delta Descriptors (2020)</a></p>\n<p><a href=\"https://github.com/oravus/CoarseHash\">CoarseHash (2020)</a></p>\n<p><a href=\"https://github.com/oravus/lostX\">LoST (2018)</a></p>\n","name":"seq2single","type":"code","url":"https://github.com/oravus/seq2single","id":"seq2single_code","image":"gitPics/illustration.png","_images":["/_next/static/images/illustration-73bec1a3cac56819cdbea1268b711fa4.png.webp","/_next/static/images/illustration-1e185173132d7d8138449660ac905c04.png"],"src":"/content/visual_place_recognition/seq2single.md","image_position":"center"}},"__N_SSG":true}