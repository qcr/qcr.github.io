{"pageProps":{"codeData":{"content":"<h1>Fast and Robust Bio-inspired Teach and Repeat Navigation</h1>\n<p><a href=\"https://arxiv.org/abs/2010.11326\"><picture><img alt=\"arXiv\" src=\"https://img.shields.io/badge/arXiv-2010.11326-a.svg?style=flat-square\"></picture></a>\n<a href=\"https://doi.org/10.1109/IROS51168.2021.9636334\"><picture><img alt=\"arXiv\" src=\"https://img.shields.io/badge/DOI-10.1109/IROS51168.2021.9636334-a.svg?style=flat-square\"></picture></a>\n<a href=\"./LICENSE\"><picture><img alt=\"License: BSD-2-Clause\" src=\"https://img.shields.io/github/license/QVPR/teach-repeat.svg?style=flat-square\"></picture></a>\n<a href=\"https://github.com/Tobias-Fischer/ensemble-event-vpr/stargazers\"><picture><img alt=\"stars\" src=\"https://img.shields.io/github/stars/QVPR/teach-repeat.svg?style=flat-square\"></picture></a>\n<a href=\"https://github.com/QVPR/teach-repeat/issues\"><picture><img alt=\"GitHub issues\" src=\"https://img.shields.io/github/issues/QVPR/teach-repeat?style=flat-square\"></picture></a>\n<a href=\"./README.md\"><picture><img alt=\"GitHub repo size\" src=\"https://img.shields.io/github/repo-size/QVPR/teach-repeat.svg?style=flat-square\"></picture></a>\n<a href=\"https://qcr.github.io\"><picture><img alt=\"QUT Centre for Robotics Open Source\" src=\"https://img.shields.io/badge/collection-QUT%20Robotics-%23043d71?style=flat-square\"></picture></a></p>\n<p>This repository contains code for a low compute teach and repeat navigation approach which only requires monocular vision and wheel odometry. Teach the robot a route by teleoperation, then the robot will be able to repeat it - robust to lighting variation and moderate environmental changes. For full details see our <a href=\"https://doi.org/10.1109/IROS51168.2021.9636334\">IROS2021 paper</a>, available on <a href=\"https://arxiv.org/abs/2010.11326\">arXiv</a>. You can view the conference presentation <a href=\"https://qvpr.github.io/teach-repeat/\">here</a> as well as other multimedia material and a full 550 metre outdoor run.</p>\n<p><video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.webp\"><source src=\"/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.webm\" type=\"video/webm\"><source src=\"/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.mp4\" type=\"video/mp4\">Overview of approach</video></p>\n<h2>License and attribution</h2>\n<p>If you use the code in this repository, please cite <a href=\"https://doi.org/10.1109/IROS51168.2021.9636334\">our paper</a>. The code is available under the <a href=\"./LICENSE\">BSD-2-Clause License</a>.</p>\n<pre class=\"language-bibtex\"><code class=\"language-bibtex\">@inproceedings{dallostoFastRobustBioinspired2021,\n      title = {Fast and {{Robust Bio-inspired Teach}} and {{Repeat Navigation}}},\n      booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},\n      author = {Dall'Osto, Dominic and Fischer, Tobias and Milford, Michael},\n      year = {2021},\n      month = sep,\n      pages = {500--507},\n      publisher = {{IEEE}},\n      address = {{Prague, Czech Republic}},\n      doi = {10.1109/IROS51168.2021.9636334},\n}\n</code></pre>\n<h2>Setup and use</h2>\n<p>This approach can be used with any mobile robot with a monocular camera and odometry source.</p>\n<p>For the teach run, run both the <code>data_collect.py</code> and <code>data_save.py</code> nodes. Teleoperate the robot along the desired route and the teach run (odometry poses and images) will be recorded to a specified folder.</p>\n<p>For the repeat run, use <code>image_matcher.py</code> and <code>localiser.py</code>. The localiser will publish <code>Goal</code> messages on the topic <code>goal</code>, containing a goal to navigate to in the robot's odometry frame. An example <code>drive_to_pose_controller</code> is used here, but can be replaced with another controller as required.</p>\n<p>In both cases, remap the <code>odom</code> and <code>image</code> topics to those provided by the robot. Note, the published odometry must also contain an integrated pose estimate.</p>\n<p>Essential parameters for these nodes are shown below. Other parameters exist to save additional diagnostic data, or to wait for a ready signal before starting - if the robot needs to run a setup procedure for example. These are shown in the nodes and example usage is shown in the provided launch files.</p>\n<h3>Global parameters</h3>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>/data_load_dir</td>\n<td>directory in which the teach runs are saved</td>\n<td>~/miro/data</td>\n</tr>\n<tr>\n<td>/data_save_dir</td>\n<td>directory in which to save the results of a repeat run</td>\n<td>~/miro/data/follow-straight_tests/5</td>\n</tr>\n<tr>\n<td>/image_resize_width</td>\n<td>width to resize images before comparison</td>\n<td>115</td>\n</tr>\n<tr>\n<td>/image_resize_height</td>\n<td>height to resize images before comparison</td>\n<td>44</td>\n</tr>\n<tr>\n<td>/patch_size</td>\n<td>patch size to use for patch normalisation</td>\n<td>(9,9)</td>\n</tr>\n<tr>\n<td>/goal_pose_separation</td>\n<td>distance between goals, should match <code>~distance_threshold</code> in <code>data_collect.py</code></td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>/image_field_of_view_width_deg</td>\n<td>horizontal field of view of images (degrees)</td>\n<td>175.2</td>\n</tr>\n<tr>\n<td>/wait_for_ready</td>\n<td>whether the localiser waits for a service signal 'ready_localiser' before starting, allowing robot initialisation</td>\n<td>false</td>\n</tr>\n</tbody>\n</table>\n<h3>Parameters for <code>data_collect.py</code></h3>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Description</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>~distance_threshold</td>\n<td>distance (metres) travelled from the previous pose after which a new pose is stored in the teach map</td>\n<td>0.2</td>\n</tr>\n<tr>\n<td>~angle_threshold_deg</td>\n<td>angular distance (degrees) travelled from the previous pose after which a new pose is stored in the teach map</td>\n<td>15.0</td>\n</tr>\n</tbody>\n</table>\n<h3>Parameters for <code>data_save.py</code></h3>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Description</th>\n<th>Example Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>~save_dir</td>\n<td>directory in which to save the teach run</td>\n<td>~/miro/data</td>\n</tr>\n<tr>\n<td>~timestamp_folder</td>\n<td>whether to timestamp the folder name of the teach run, so multiple runs can be performed without overwriting</td>\n<td>true</td>\n</tr>\n</tbody>\n</table>\n<h3>Parameters for <code>localiser.py</code></h3>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Description</th>\n<th>Default Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>~rotation_correction_gain</td>\n<td>proportional gain term to use for rotation corrections, $K_\\theta$, shouldn't need to be tuned</td>\n<td>0.01</td>\n</tr>\n<tr>\n<td>~path_correction_gain</td>\n<td>proportional gain term to use for along-path corrections, $K_p$, shouldn't need to be tuned</td>\n<td>0.01</td>\n</tr>\n<tr>\n<td>~stop_at_end</td>\n<td>whether the robot should stop at the end of the route, otherwise it assumes the route is circular and restarts from the beginning</td>\n<td>true</td>\n</tr>\n<tr>\n<td>~discrete-correction</td>\n<td>reduce compute by only performing a correction at each goal pose, not continually</td>\n<td>false</td>\n</tr>\n<tr>\n<td>~search-range</td>\n<td>how many teach images to search either side of the current to perform along-path correction</td>\n<td>1</td>\n</tr>\n<tr>\n<td>~global_localisation_init</td>\n<td>when initialising, find the closest matching teach image to the current and start the route from there, otherwise start at the first goal</td>\n<td>false</td>\n</tr>\n<tr>\n<td>~min_init_correlation</td>\n<td>minimum correlation with a teach image at initialisation, otherwise the robot thinks it's not on the path and doesn't start repeating</td>\n<td>0.0</td>\n</tr>\n</tbody>\n</table>\n<h2>Overview of approach</h2>\n<h3>Teach run</h3>\n<p>First the robot needs to be taught a route via teleoperation. At regular distance intervals along the path the dead-reckoning position and and image will be saved, resulting in a topometric map of the route. Images are patch normalised to increase robustness to lighting variation.</p>\n<p><video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/teach-run-cbadf24f03abf12d291128d6d00f5d0e.webp\"><source src=\"/_next/static/images/teach-run-cbadf24f03abf12d291128d6d00f5d0e.webm\" type=\"video/webm\"><source src=\"/_next/static/images/teach-run-cbadf24f03abf12d291128d6d00f5d0e.mp4\" type=\"video/mp4\">Teach run visualisation</video></p>\n<h3>Repeat run</h3>\n<p>Having learnt a route, the robot can robustly repeat it. The robot initially follows the sequence of odometry poses stored during the teach run, but errors accumulate in this approach over time. Images are compared between the teach and repeat routes to make corrections to the route.</p>\n<h4>Correction overview</h4>\n<p>Both rotational and lateral path errors result in horizontal image offsets that can't be distinguished, but this is not a problem because both require the same correction response. However, moving along the path can also horizontal image offsets. These must be accounted for by interpolating between the previous and next goal images.</p>\n<p><video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/correction-ambiguity-568e6ee37dd381163f02cb8599f1d364.webp\"><source src=\"/_next/static/images/correction-ambiguity-568e6ee37dd381163f02cb8599f1d364.webm\" type=\"video/webm\"><source src=\"/_next/static/images/correction-ambiguity-568e6ee37dd381163f02cb8599f1d364.mp4\" type=\"video/mp4\">Correction ambiguity overview</video></p>\n<h4>Orientation correction</h4>\n<p>If an orientation error is detected by comparing teach and repeat images, an associated path correction is performed, modulated by a constant gain factor. This correction causes the robot to steer back onto the path.</p>\n<p><video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/orientation-correction-b37aac230b4914472b3b4d5dc4b893dc.webp\"><source src=\"/_next/static/images/orientation-correction-b37aac230b4914472b3b4d5dc4b893dc.webm\" type=\"video/webm\"><source src=\"/_next/static/images/orientation-correction-b37aac230b4914472b3b4d5dc4b893dc.mp4\" type=\"video/mp4\">Orientation correction</video></p>\n<h4>Along-path correction</h4>\n<p>Repeat images are compared to teach images within a certain search range of the current goal. If correlation values are stronger to images ahead or behind the robot's current estimated position, and along-path correction is performed. In this case, the goal is pulled towards the robot so it will be reached faster, allowing the estimated position to \"catch up\" to the real position.</p>\n<p><video autoplay=\"\" muted=\"\" loop=\"\" poster=\"/_next/static/images/along-path-correction-60ecaeca4eec79f0075a5d55279c77e3.webp\"><source src=\"/_next/static/images/along-path-correction-60ecaeca4eec79f0075a5d55279c77e3.webm\" type=\"video/webm\"><source src=\"/_next/static/images/along-path-correction-60ecaeca4eec79f0075a5d55279c77e3.mp4\" type=\"video/mp4\">Along-path correction</video></p>\n<h2>Examples of running teach and repeat for the robots we used</h2>\n<h3>Running teach on Miro</h3>\n<ul>\n<li><code>roslaunch teach_repeat data_collection_miro.launch</code></li>\n</ul>\n<h3>Running repeat on Miro</h3>\n<ul>\n<li><code>roslaunch teach_repeat data_matching_miro.launch</code></li>\n</ul>\n<h3>Running teach on Jackal</h3>\n<ul>\n<li><code>rosnode kill twist_mux</code> (optional, only required for comparison with bearnav)</li>\n<li><code>roslaunch slam_toolbox localization.launch</code> (optional, only required for quantitative analysis)</li>\n<li><code>roslaunch stroll_bearnav mapping-core-jackal.launch</code> (optional, only required for comparison with bearnav)</li>\n<li><code>roslaunch stroll_bearnav mapping-gui-jackal.launch</code>  (optional, only required for comparison with bearnav)</li>\n<li><code>roslaunch teach_repeat data_collection_jackal.launch</code></li>\n</ul>\n<h3>Running repeat on Jackal</h3>\n<ul>\n<li><code>roslaunch slam_toolbox localization.launch</code> (optional, only required for quantitative analysis)</li>\n<li><code>roslaunch teach_repeat data_matching_jackal.launch</code></li>\n</ul>\n<h3>Running Bearnav repeat on Jackal</h3>\n<ul>\n<li><code>roslaunch slam_toolbox localization.launch</code></li>\n<li><code>roslaunch stroll_bearnav navigation-core-jackal.launch</code></li>\n<li><code>roslaunch stroll_bearnav navigation-gui-jackal.launch</code></li>\n</ul>\n","name":"Visual Teach and Repeat","type":"code","url":"https://github.com/QVPR/teach-repeat","id":"teach_repeat","image":"assets/outdoor-run.gif","_images":["/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.webm","/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.mp4","/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.webp","/_next/static/images/outdoor-run-c6d0f9054f19ca3ca4a9c32ae5089b50.jpg"],"src":"/content/visual_place_recognition/teach-repeat.md","image_position":"center"}},"__N_SSG":true}